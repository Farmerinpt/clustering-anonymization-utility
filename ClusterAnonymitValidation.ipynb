{"cells":[{"cell_type":"code","source":["%matplotlib inline\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import os\n","import glob\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import davies_bouldin_score\n","from sklearn.metrics import calinski_harabasz_score\n","from sklearn.metrics import silhouette_samples, silhouette_score\n"],"metadata":{"id":"PUlmoA4hxEf2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def clean_dataset(df):\n","  \"\"\" Remove lines with nonsensical values, if any.\n","   Keyword arguments: df is a Pandas' library dataframe.\n","  \"\"\"\n","  assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n","  df.dropna(inplace=True)\n","  indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf,\"*\"]).any(1)\n","  return df[indices_to_keep].astype(np.float64)"],"metadata":{"id":"VWUA2suxO1Nz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plotValidityIndices(ss, dbs, chs, maximumClusters):\n","  \"\"\" Shows lineplots of three different cluster validity indices.\n","  The values are for a number of clusters ranging from 2 to maximumClusters\n","   Keyword arguments: \n","   - ss is a numpy array with the average Silhouette Score.\n","   - dbs is a numpy array with the Davies Bouldin score.\n","   - chs is a numpy array with the Calinski Harabasz score.\n","  \"\"\"\n","  range_n_clusters = range(2, maximumClusters+1)\n","  fig = plt.figure(figsize=(21,4))\n","\n","  fig.add_subplot(131)\n","  plt.grid(True)\n","  plt.plot(range_n_clusters, ss,'b-',label='Silhouette Score')\n","  plt.xlabel(\"Number of cluster\")\n","  plt.ylabel(\"Silhouette Score\")\n","  plt.legend()\n","\n","  fig.add_subplot(132)\n","  plt.grid(True)\n","  plt.plot(range_n_clusters, dbs,'b-',label='Davies Bouldin score')\n","  plt.xlabel(\"Number of cluster\")\n","  plt.ylabel(\"Davies Bouldin score\")\n","  plt.legend()\n","\n","  fig.add_subplot(133 )\n","  plt.grid(True)\n","  plt.plot(range_n_clusters, chs,'b-',label='Calinski Harabasz score')\n","  plt.xlabel(\"Number of cluster\")\n","  plt.ylabel(\"Calinski Harabasz score\")\n","  plt.legend()\n","  plt.show()"],"metadata":{"id":"jeBxrJxFPAui"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def multiCluster(dataFrame, maximumClusters, plot = True):\n","  \"\"\" Applies the KMeans clustering algorithm to a Pandas dataframe \n","  varying the number of clusters to be formed (ranging from 2 to maximumClusters). \n","  Also computes three different cluster validity indices.\n","   Keyword arguments: \n","   - dataFrame: Pandas dataframe with the data to be clustered.\n","   - maximumClusters: defines the range of different groups to be formed.\n","   - plot: boolean that determines if the indices are to be ploted.\n","   Returns:\n","   - ss: a numpy array with the Silhouette Score for the different number of clusters.\n","   - dbs: a numpy array with the Davies Bouldin score for the different clusters.\n","   - chs: a numpy array with the Calinski Harabasz score the different clusters.\n","  \"\"\"\n","  ss = []\n","  dbs = []\n","  chs = []\n","  range_n_clusters = range(2,maximumClusters+1)\n"," \n","  for n_clust in range_n_clusters:\n","    # Initialize the clusterer with n_clust value and a random generator\n","    # seed of 123 for reproducibility.\n","    clusterer = KMeans(n_clusters=n_clust, random_state=123)\n","    cluster_labels = clusterer.fit_predict(dataFrame)\n","\n","    silhouette_avg = silhouette_score(dataFrame, cluster_labels)\n","    db_avg = davies_bouldin_score(dataFrame, cluster_labels)\n","    ch_avg = calinski_harabasz_score(dataFrame, cluster_labels)\n","    ss.append(silhouette_avg)\n","    dbs.append(db_avg)\n","    chs.append(ch_avg)\n","\n","  if plot:\n","    plotValidityIndices(ss, dbs, chs, maximumClusters)\n","\n","  return ss, dbs, chs"],"metadata":{"id":"cSh-C4ZbO6g1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZeI9WF-o4WlK"},"outputs":[],"source":["\"\"\" Load the datasets.\n","The following code is obfuscated and should be optimized for your own needs.\n","In this case we are assuming that C0 always refers to the original data.\n","For the study at hand C2,C3,C4,C5 are 4 different datasets that result from\n","4 different anonymization techniques applied to C0.\n","\"\"\"\n","path = '/a/b/c/'\n","#If there are unecessary or unused columns:\n","dropColumns = ['col_x','col_y', 'col_z']\n","#If the files are available in xlsx format:\n","C0 = pd.DataFrame(pd.read_excel(path + 'originalData.xlsx')).drop(dropColumns, axis = 1)\n","C2 = pd.DataFrame(pd.read_excel(path + 'anonym1.xlsx')).drop(dropColumns, axis = 1)\n","C3 = pd.DataFrame(pd.read_excel(path + 'anonym2.xlsx')).drop(dropColumns, axis = 1)\n","C4 = pd.DataFrame(pd.read_excel(path + 'anonym3.xlsx')).drop(dropColumns, axis = 1)\n","C5 = pd.DataFrame(pd.read_excel(path + 'anonym4.xlsx')).drop(dropColumns, axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cW6ho5ck6uGX"},"outputs":[],"source":["\"\"\" Data cleaning.\n","\"\"\"\n","X_C0 = clean_dataset(C0).to_numpy()\n","X_C2 = clean_dataset(C2).to_numpy()\n","X_C3 = clean_dataset(C3).to_numpy()\n","X_C4 = clean_dataset(C4).to_numpy()\n","X_C5 = clean_dataset(C5).to_numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OcLgKIEb_NtU"},"outputs":[],"source":["\"\"\" Data normalization.\n","\"\"\"\n","scaler = StandardScaler()\n","S_C0 = np.c_[scaler.fit_transform(X_C0)]\n","S_C2 = np.c_[scaler.fit_transform(X_C2)]\n","S_C3 = np.c_[scaler.fit_transform(X_C3)]\n","S_C4 = np.c_[scaler.fit_transform(X_C4)]\n","S_C5 = np.c_[scaler.fit_transform(X_C5)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CuSEIBcmXjiz"},"outputs":[],"source":["\"\"\" Define the maximum number of clusters.\n","A good first approximation could be the square root of the original dataset cardinality.\n","\"\"\"\n","maxNumberOfClusters = 40\n","\n","\"\"\" Apply the clustering+validity pipeline to the original (S_C0) and \n","to the anonymized data.\n","\"\"\"\n","scaledData = [S_C0, S_C2, S_C3, S_C4, S_C5]\n","for sd in scaledData:\n","  ss, dbs, chs  =  multiCluster(sd, maxNumberOfClusters, True)\n","\n"]}],"metadata":{"colab":{"name":"ClusterAnonymitValidation.ipynb","provenance":[{"file_id":"19ZgqT5acoBN33Ud0f5TscOkQG8aMRJIM","timestamp":1647944006545}],"mount_file_id":"19ZgqT5acoBN33Ud0f5TscOkQG8aMRJIM","authorship_tag":"ABX9TyM1jY6+/uJCCwG3fTvW5not"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}